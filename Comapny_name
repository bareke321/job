from fuzzywuzzy import fuzz, process
import pandas as pd

# read in the datasets
df1 = pd.read_csv('companies1.csv') # first dataset with company names and IDs
df2 = pd.read_csv('companies2.csv') # second dataset with similar company names

# create a function to find the best match for a company name in df2
def find_best_match(name, df):
    matches = process.extract(name, df['name'], limit=1, scorer=fuzz.token_sort_ratio)
    if len(matches) > 0:
        match_name = matches[0][0]
        match_score = matches[0][1]
        return df[df['name'] == match_name].iloc[0]
    else:
        return None

# match the companies from df1 to the closest company in df2
df1['matched_company'] = df1['name'].apply(lambda x: find_best_match(x, df2))

# output the results
for i, row in df1.iterrows():
    print(f"{row['name']} ({row['id']}) matched with {row['matched_company']['name']} ({row['matched_company']['id']})")
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer

# define a stemmer to reduce words to their root form
stemmer = SnowballStemmer('english')

# define a list of common company words to remove
common_words = ['ltd', 'ag', 'inc', 'corp', 'llc']

def normalize_company_name(name):
    # tokenize the company name and stem each word
    tokens = word_tokenize(name.lower())
    stemmed_tokens = [stemmer.stem(token) for token in tokens]

    # remove common company words
    filtered_tokens = [token for token in stemmed_tokens if token not in common_words]

    # join the filtered tokens back into a string
    filtered_name = ' '.join(filtered_tokens)
    
    return filtered_name
